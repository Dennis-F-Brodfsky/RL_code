[1] [A survey of monte carlo tree search methods](http://repository.essex.ac.uk/4117/1/MCTS-Survey.pdf)
[2] [Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization](https://www.researchgate.net/profile/Yuval-Tassa/publication/261353847_Synthesis_and_stabilization_of_complex_behaviors_through_online_trajectory_optimization/links/55bf570608aed621de127688/Synthesis-and-stabilization-of-complex-behaviors-through-online-trajectory-optimization.pdf)
[3] [Differential dynamic programming](https://www.sciencedirect.com/science/article/pii/B9780120127108500108)
[4] [Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models](https://proceedings.neurips.cc/paper/2018/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf)
[5] [A Model-Based and Data-Efficient Approach to Policy Search](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.303.7735&rep=rep1&type=pdf)
[6] [Neural Network Dynamics for Model Based Deep Reinforcement Learning with Model-Free Fine-Tuning](https://arxiv.org/pdf/1708.02596)
[7] [Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models](https://proceedings.neurips.cc/paper/2018/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf)
[8] [Model-based value estimation for efficient model-free reinforcement learning](https://arxiv.org/pdf/1803.00101)
[9] [Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion](https://proceedings.neurips.cc/paper/2018/file/f02208a057804ee16ac72ff4d3cec53b-Paper.pdf)
[10] [Embed to Control:A Locally Linear Latent Dynamics Model for Control from Raw Images](https://proceedings.neurips.cc/paper/2015/file/a1afc58c6ca9540d057299ec3016d726-Paper.pdf)
[11] [SOLAR:Deep Structured Latent Representations for Model-Based Reinforcement Learning](http://proceedings.mlr.press/v97/zhang19m/zhang19m.pdf)
[12] [Deep Visual Foresight for Planning Robot Motion](https://arxiv.org/pdf/1610.00696.pdf%20http://arxiv.org/abs/1610.00696)
[13] [Self-Supervised Visual Planning with Temporal Skip Connections](https://proceedings.mlr.press/v78/frederik%20ebert17a/frederik%20ebert17a.pdf)
[14] [Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics](https://proceedings.neurips.cc/paper/2014/file/6766aa2750c19aad2fa1b32f36ed4aee-Paper.pdf)
[15] [Flexible Model-Based Policy Search Robust to the Curse of Chaos](http://proceedings.mlr.press/v80/parmas18a/parmas18a.pdf)
[16] [Integrated architectures for learning, planning, and reacting based on approximating dynamic programming](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.51.7362&rep=rep1&type=pdf)
[17] [Continuous deep Q-learning with model-based acceleration](http://proceedings.mlr.press/v48/gu16.pdf)
[18] [Model-based value expansion](https://arxiv.org/pdf/1803.00101)
[19] [When to trust your model: model-based policy optimization](https://proceedings.neurips.cc/paper/2019/file/5faf461eff3099671ad63c6f3f094f7f-Paper.pdf)
[20] [Distilling the Knowledge in a Neural Network](https://openaccess.thecvf.com/content/WACV2022/supplemental/Jeong_BiHPF_Bilateral_High-Pass_WACV_2022_supplemental.zip)
[21] [Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning](https://arxiv.org/pdf/1511.06342.pdf%20http://arxiv.org/abs/1511.06342)
[22] [Divide and Conquer Reinforcement Learning](https://arxiv.org/pdf/1711.09874)
[23] [End-to-End Training of Deep Visuomotor Policies](https://www.jmlr.org/papers/volume17/15-522/15-522.pdf)
[24] [Policy distillation](https://arxiv.org/pdf/1511.06295.pdf?fbclid=IwAR2P9PcMDwVPLYL_quZlaJIiz5nqhpjIfFWOQbQfFdlVU5Y4Q3WHF1_gWgw)
[25] [Distral: Robust Multitask Reinforcement Learning](https://proceedings.neurips.cc/paper/2017/file/0abdc563a06105aee3c6136871c9f4d1-Paper.pdf)
